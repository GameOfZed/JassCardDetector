{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Function to download dataset\n",
    "def download_dataset(dataset_path, kaggle_path):\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "        print(\"Downloading dataset...\")\n",
    "        kaggle.api.dataset_download_files(kaggle_path, path=dataset_path, unzip=True)\n",
    "    else:\n",
    "        print(\"Dataset already exists. Skipping download.\")\n",
    "\n",
    "# Download Jass Card Dataset\n",
    "dataset_path = '../../Data/Processed/database'\n",
    "kaggle_path = 'pbegert/swiss-jass-cards'\n",
    "download_dataset(dataset_path, kaggle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E_0': 0, 'E_1': 1, 'E_2': 2, 'E_3': 3, 'E_4': 4, 'E_5': 5, 'E_6': 6, 'E_7': 7, 'E_8': 8, 'H_0': 9, 'H_1': 10, 'H_2': 11, 'H_3': 12, 'H_4': 13, 'H_5': 14, 'H_6': 15, 'H_7': 16, 'H_8': 17, 'S_0': 18, 'S_1': 19, 'S_2': 20, 'S_3': 21, 'S_4': 22, 'S_5': 23, 'S_6': 24, 'S_7': 25, 'S_8': 26, 'K_0': 27, 'K_1': 28, 'K_2': 29, 'K_3': 30, 'K_4': 31, 'K_5': 32, 'K_6': 33, 'K_7': 34, 'K_8': 35}\n"
     ]
    }
   ],
   "source": [
    "def create_card_mapping():\n",
    "    suits = ['E', 'H', 'S', 'K']  # Ecke, Herz, Schaufel, Kreuz\n",
    "    values = ['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "    mapping = {}\n",
    "    class_id = 0\n",
    "    for suit in suits:\n",
    "        for value in values:\n",
    "            mapping[f'{suit}_{value}'] = class_id\n",
    "            class_id += 1\n",
    "    print(mapping)\n",
    "    return mapping\n",
    "\n",
    "card_mapping = create_card_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class JassCardDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.images = [img for img in os.listdir(directory) if img.endswith(('.png', '.jpg', '.jpeg'))]  # Filter for image files\n",
    "        self.mapping = create_card_mapping()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.directory, self.images[idx])\n",
    "        image = Image.open(img_name)\n",
    "        label = self.extract_label(self.images[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def extract_label(self, filename):\n",
    "        # Attempt to extract the card ID from the filename\n",
    "        card_id = filename.split('_')[0] + '_' + filename.split('_')[1]\n",
    "        label = self.mapping.get(card_id, None)\n",
    "\n",
    "        # Debugging print statements\n",
    "        if label is None:\n",
    "            print(f\"Unmapped label for file: {filename}, Extracted card_id: {card_id}\")\n",
    "\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E_0': 0, 'E_1': 1, 'E_2': 2, 'E_3': 3, 'E_4': 4, 'E_5': 5, 'E_6': 6, 'E_7': 7, 'E_8': 8, 'H_0': 9, 'H_1': 10, 'H_2': 11, 'H_3': 12, 'H_4': 13, 'H_5': 14, 'H_6': 15, 'H_7': 16, 'H_8': 17, 'S_0': 18, 'S_1': 19, 'S_2': 20, 'S_3': 21, 'S_4': 22, 'S_5': 23, 'S_6': 24, 'S_7': 25, 'S_8': 26, 'K_0': 27, 'K_1': 28, 'K_2': 29, 'K_3': 30, 'K_4': 31, 'K_5': 32, 'K_6': 33, 'K_7': 34, 'K_8': 35}\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "jass_dataset = JassCardDataset(directory=dataset_path, transform=transform)\n",
    "# Define the size of the validation set\n",
    "\n",
    "validation_size = int(0.2 * len(jass_dataset))  # 20% for validation\n",
    "train_size = len(jass_dataset) - validation_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, validation_dataset = random_split(jass_dataset, [train_size, validation_size])\n",
    "\n",
    "# Create DataLoaders for both training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # Calculate the size of the flattened features after the final pooling layer\n",
    "        # After conv1 and pool: 64x64 -> 32x32\n",
    "        # After conv2 and pool: 32x32 -> 16x16\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, len(card_mapping))  # Number of unique card classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 2.4069318771362305, Validation Loss: 2.3778592054394707, Accuracy: 0.28051001821493626\n",
      "Epoch 1, Training Loss: 1.5306581258773804, Validation Loss: 1.5281315955562869, Accuracy: 0.5063752276867031\n",
      "Epoch 2, Training Loss: 1.3008038997650146, Validation Loss: 1.342079598834549, Accuracy: 0.5701275045537341\n",
      "Epoch 3, Training Loss: 1.1528294086456299, Validation Loss: 1.242827768775, Accuracy: 0.6015482695810564\n",
      "Epoch 4, Training Loss: 1.3179723024368286, Validation Loss: 1.470191186752872, Accuracy: 0.5669398907103825\n",
      "Epoch 5, Training Loss: 0.9243032336235046, Validation Loss: 1.3188903996909873, Accuracy: 0.6174863387978142\n",
      "Epoch 6, Training Loss: 1.2334765195846558, Validation Loss: 1.5022350748380024, Accuracy: 0.5919854280510018\n",
      "Epoch 7, Training Loss: 1.2498934268951416, Validation Loss: 1.5250528351120327, Accuracy: 0.6256830601092896\n",
      "Epoch 8, Training Loss: 1.3657132387161255, Validation Loss: 1.5320464726807415, Accuracy: 0.6461748633879781\n",
      "Epoch 9, Training Loss: 1.3949769735336304, Validation Loss: 1.6838851346485857, Accuracy: 0.6306921675774135\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    # Training loop\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    valid_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in validation_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_targets.extend(targets)\n",
    "            all_predictions.extend(predicted)\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    print(f\"Epoch {epoch}, Training Loss: {loss.item()}, Validation Loss: {valid_loss / len(validation_loader)}, Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model, 'jass_card_classifier_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
